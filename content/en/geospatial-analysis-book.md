---
lang: en
slug: geospatial-analysis-book
title: The book I start to write about cloud-based geospatial analysis
date:  2017-05-01
tags: book
---
<!-- more -->
![](http://oouh9u8nz.bkt.gdipper.com//geospatial-analysis-book.jpg)

The editors from [packtpub](https://www.packtpub.com/) wrote to me a few weeks ago, asking whether I would like to write a short book about the application of [Google Earth Engine](https://earthengine.google.com/). I am not sure whether I want to do it, but I would like to write down some thoughts I have for now.

# Preface
Geospatial analysis is the combination of statistical analysis, computational geometry, and image processing applied to data which are tied to the Earth thus far (maybe in the future to other planets). When I was a young student, I started to learn how to do RGB bands combination in GIS software, like ArcGIS or ERDAS. These are called ‘image processing’ software or ‘spatial modeling’ tools because that is exactly what they can do. Right now, I am delighted to see on the homepage of ERDAS that they have started calling themselves a ‘Geospatial Data Authoring System’ provider. This is a better term, since more and more complex operations and data are included in geospatial analysis. People are trying to process and analyze all kind of data with spatial information and attributes.

Analyst used to be dependent on expensive commercial applications like ArcGIS, ERDAS or ENVI, or packages from the open source community (like QGIS, GDAL, etc.), to perform geospatial analysis for more than 20 years. Millions of images have been processed following the same strategy: locate data (through online and physical image archives); acquire data (through internet, FTP, or physical media); process data (usually on a single workstation, only occasionally using parallel programming on high-performance computing (HPC) systems); visualizing and analyzing results (again on a single workstation); and delivering a final product (usually some form of digital map with accompanying metadata and spatial statistics). All these steps required a great deal of time, and it became apparent that often computing power and operational workflows were the limiting factors in large scale geospatial analysis, not datasets or algorithms.

There are still a lot of things to be optimized/visualized beyond processing, like feature exploration, model validation, and tuning. Most of them are still painful tasks not because we don’t have dataset or algorithm, not because we don’t know how to do those things, but because we don’t know how to make them reproducible, stable and scalable. 

Cloud computing is the buzzword today in the IT world. The most appropriate definition of cloud computing is provided by Borko Furht of Florida Atlantic University, who defines it as "a new style of computing in which dynamically scalable and often virtualized resources are provided as a service over the Internet." Cloud computing is the best way to resolve two obstacles in the geospatial analysis in nowadays: sharing and scaling. 

# Sharing of data and services
The underlying philosophy behind the cloud is the sharing of data and services. There are two approaches to this from users' view: public and private clouds. In a public cloud, the provider has total control of data and services; users can access data and services for realizing their applications. The control on data and services and the right to access are decided by the provider. Public clouds are usually off-premises and run by third parties. Google Maps is perhaps the most visible example of such a public cloud. However, there are many instances where an organization needs to be on the cloud but has security and reliability issues. In such cases, private clouds are the answer. A private cloud is on-premises and run by a wing of the organization. It is accessible to users within that organization. Many people criticize the concept of a private cloud because it provides none of the benefits of cloud architecture except web access through virtualization. For community clouds, several related organizations can get together and create a cloud infrastructure for their use, a typical system being Google's Apps.gov for US government departments. These are accessed as private, public, hybrid or community cloud services, depending upon the organization’s need for security, collaboration, and ownership.

Google Earth Engine’s public data catalog includes a variety of standard Earth science raster datasets. The most popular ones include Landsat, Sentinel, MODIS, Climate Dataset and Land Cover Maps. On the cloud, you don’t actually order or download these datasets. Virtually they are available through the online developing platform. You can describe what you plan to do with these datasets in a short javascript code, and manipulate objects on the server by manipulating client-side “proxy” objects. The real computing will be carried out on the server side. All the traditional functions and library are now available through web-services. And you can also share your codes and output layers to the whole community.

# Scaling 
Scaling, from an IT resource perspective, represents the ability of the IT resource to handle increased or decreased usage demands. People used to build huge computing system to make thing possible. For example, the [Pleiades Supercomputer](https://www.nas.nasa.gov/hecc/resources/pleiades.html) represents NASA's state-of-the-art technology for meeting the agency's supercomputing requirements, enabling NASA scientists and engineers to conduct modeling and simulation for NASA missions. If you are an experienced programmer with HPC knowledge, you can definitely build some strong data pipeline to dig [into?] your massive dataset. However, these computing resources are highly limited to the public because of the learning curve, security requirement and lack of infrastructure. People need a well-defined infrastructure to work with. It should be flexible enough to address various questions; it should also be well abstracted, so users don’t need to fight with too many details. In the end, scaling is a question highly bound with lower-level implementation of the computing system, which means you can’t make an algorithm scalable by simply putting it like that; you have to know how your resources (memory, storage, CPUs) are organized in your system, which makes everything complex. However, a cloud-based geospatial platform means it provides a general solution to these questions so that people can develop on top of it.

So, while traditional GIS is installed on your desktop or server, Cloud GIS makes use of the flexibility of the cloud environment for data capture, visualization, analysis, and sharing. Although GIS has been a late adopter of cloud technology, the many advantages are compelling organizations to shift their geospatial functions to the cloud. Cloud-based tools are accessed through a web-based geographic information system. There are quite a few new things with cloud-based geospatial analysis. If you are a student or researcher in this field, you can undoubtedly get some ideas from reading this book, and keep an eye on the changes in this topic. 

I would like to start with a short chapter overviewing general geospatial analysis at different levels; we need to apply different tools for questions at different levels. Most of the time we can find open source tools to our questions. After that we will introduce Google Earth Engine; so far for some specific questions, I have only found a solution using it. It is under development and not open source; and it looks like a huge, ambitious project with a lot of uncertainties -- just like the ones people have had many times in history.  But as we know, some of them did change the world in the end, in a good way. [I think this ending needs to be more strongly positive]
